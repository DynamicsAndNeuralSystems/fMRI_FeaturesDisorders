---
title: "Step 5: ROI+Feature Classification Analysis"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=F, message=F)
```

### Source functions
```{r}
source("../helper_functions/visualization_functions.R")
source("ROI_and_feature_analysis_functions.R")
source("D:/Virtual_Machines/Shared_Folder/github/theft-trent/R/fit_multi_feature_classifier.R")
theme_set(theme_cowplot())
rdata_path <- "D:/Virtual_Machines/Shared_Folder/PhD_work/data/scz/UCLA/Rdata/"
```

## In-sample SVM classification

### Simple in-sample linear SVM

We will start with a simple linear SVM classifier using all 22 features.

```{r}
# Compare all three noise processing methods
noise_procs = c("AROMA+2P", "AROMA+2P+GMR", "AROMA+2P+DiCER")

# Use linear SVM
svm_kernel = "linear"

# Run theft's multivariable classifier on each ROI and save to an RDS object
# If the RDS object doesn't already exist, otherwise load it in
if (!file.exists(paste0(rdata_path, "UCLA_multivar_feature_ROI_res_svmLinear_in_sample.Rds"))) {
  feature_ROI_wise_SVM_in_sample <- run_in_sample_ksvm_by_feature_and_ROI(rdata_path = rdata_path,
                                                              svm_kernel = svm_kernel,
                                                              noise_procs = noise_procs,
                                         use_inv_prob_weighting = FALSE,
                                         upsample_minority = FALSE,
                                         downsample_majority = FALSE) %>%
    dplyr::rename("accuracy" = "Accuracy",
                  "balanced_accuracy" = "Balanced_Accuracy")
  saveRDS(feature_ROI_wise_SVM_in_sample, file=paste0(rdata_path, "UCLA_multivar_feature_ROI_res_svmLinear_in_sample.Rds"))
} else {
  feature_ROI_wise_SVM_in_sample <- readRDS(paste0(rdata_path, "UCLA_multivar_feature_ROI_res_svmLinear_in_sample.Rds"))
}
```

```{r, fig.width=7, fig.height=6}
# Plot accuracy + balanced accuracy in histograms
# Control subject proportion is highlighted for accuracy
noise_procs = c("AROMA+2P", "AROMA+2P+GMR", "AROMA+2P+DiCER")
plot_class_acc_w_props(class_res = feature_ROI_wise_SVM_in_sample,
                       cv = FALSE,
                       rdata_path = rdata_path,
                       noise_procs = noise_procs)
```
```{r, eval=F, echo=F}
# Save plot
ggsave("plots/In_Sample_Feature_Wise_linear_SVM_Multi_Region.png",
       width=6, height=5, units="in", dpi=300)
```

### In-sample linear SVM with inverse probability weighting

We can run linear SVM with the `kernlab` package to directly test sample reweighting with in-sample accuracy and balanced accuracy. 

```{r}
# Compare all three noise processing methods
noise_procs = c("AROMA+2P", "AROMA+2P+GMR", "AROMA+2P+DiCER")

# Use linear SVM
svm_kernel = "linear"

# Run in-sample ksvm with inverse probability reweighting on each catch22 and save to an RDS object
# If the RDS object doesn't already exist, otherwise load it in
if (!file.exists(paste0(rdata_path, "UCLA_multivar_feature_ROI_res_svmLinear_in_sample_inv_prob.Rds"))) {
  feature_ROI_wise_SVM_in_sample_inv_prob <- run_in_sample_ksvm_by_feature_and_ROI(rdata_path = rdata_path,
                                                                         svm_kernel = svm_kernel,
                                                                         noise_procs = noise_procs,
                                         use_inv_prob_weighting = TRUE,
                                         upsample_minority = FALSE,
                                         downsample_majority = FALSE) %>%
    dplyr::rename("accuracy" = "Accuracy",
                  "balanced_accuracy" = "Balanced_Accuracy")
  saveRDS(feature_ROI_wise_SVM_in_sample_inv_prob, file=paste0(rdata_path, "UCLA_multivar_feature_ROI_res_svmLinear_in_sample_inv_prob.Rds"))
} else {
  feature_ROI_wise_SVM_in_sample_inv_prob <- readRDS(paste0(rdata_path, "UCLA_multivar_feature_ROI_res_svmLinear_in_sample_inv_prob.Rds"))
}
```

```{r, fig.width=7, fig.height=6}
# Plot accuracy + balanced accuracy in histograms
# Control subject proportion is highlighted for accuracy
noise_procs = c("AROMA+2P", "AROMA+2P+GMR", "AROMA+2P+DiCER")
plot_class_acc_w_props(class_res = feature_ROI_wise_SVM_in_sample_inv_prob,
                       cv = FALSE,
                       rdata_path = rdata_path,
                       noise_procs = noise_procs)
```
```{r, echo=F, eval=F}
# Save plot
ggsave("plots/In_Sample_Feature_Wise_linear_SVM_Multi_Region_Inv_Prob.png",
       width=7, height=6, units="in", dpi=300)
```

By assigning each subject a weight equivalent to the inverse proportion of that subject's diagnosis, the linear SVM places a higher cost on incorrectly classifying schizophrenia subjects as controls. 

This shifts the raw accuracy down to a mean of around 0.68 across the three noise-processing methods, but the balanced accuracy increases to have an average of around 0.68 also -- compared with almost exclusively values of 0.35 previously.

This indicates that inverse probability reweighting mitigates the class imbalance issue and can be carried forward into 10-fold cross-validation linear SVM.

### In-sample linear SVM with minority class upsampling

```{r}
# Compare all three noise processing methods
noise_procs = c("AROMA+2P", "AROMA+2P+GMR", "AROMA+2P+DiCER")

# Use linear SVM
svm_kernel = "linear"

# Run theft's multivariable classifier on each ROI and save to an RDS object
# If the RDS object doesn't already exist, otherwise load it in
if (!file.exists(paste0(rdata_path, "UCLA_multivar_feature_ROI_res_svmLinear_in_sample_upsampled.Rds"))) {
  set.seed(127)
  feature_ROI_wise_SVM_in_sample_upsampled <- run_in_sample_ksvm_by_feature_and_ROI(rdata_path = rdata_path,
                                                                         svm_kernel = svm_kernel,
                                                                         noise_procs = noise_procs,
                                                                      use_inv_prob_weighting = FALSE,
                                                                      upsample_minority = TRUE,
                                                                      downsample_majority = FALSE) %>%
    dplyr::rename("accuracy" = "Accuracy",
                  "balanced_accuracy" = "Balanced_Accuracy")
  saveRDS(feature_ROI_wise_SVM_in_sample_upsampled, file=paste0(rdata_path, "UCLA_multivar_feature_ROI_res_svmLinear_in_sample_upsampled.Rds"))
} else {
  feature_ROI_wise_SVM_in_sample_upsampled <- readRDS(paste0(rdata_path, "UCLA_multivar_feature_ROI_res_svmLinear_in_sample_upsampled.Rds"))
}
```

```{r, fig.width=7, fig.height=6}
# Plot accuracy + balanced accuracy in histograms
# Control subject proportion is highlighted for accuracy
noise_procs = c("AROMA+2P", "AROMA+2P+GMR", "AROMA+2P+DiCER")
plot_class_acc_w_props(class_res = feature_ROI_wise_SVM_in_sample_upsampled,
                       cv = FALSE,
                       rdata_path = rdata_path,
                       noise_procs = noise_procs)
```
```{r, echo=F, eval=F}
# Save plot
ggsave("plots/In_Sample_Feature_Wise_linear_SVM_Multi_Region_Upsampled.png",
       width=7, height=6, units="in", dpi=300)
```

Alternatively, we can implement upsampling for the minority class (schizophrenia) such that the classes are perfectly balanced. This involves randomly sampling the schizophrenia subjects with replacement N times, where N is the number of control subjects (i.e. majority class).

This takes away the classifier's bias toward the majority class. The mean accuracy is right around 0.72, and the balanced accuracy is much higher -- with a mean of around 0.72 as well.

This indicates that minority class upsampling also mitigates the class imbalance issue and can be carried forward into 10-fold cross-validation linear SVM.

### In-sample linear SVM with majority class downsampling

```{r}
# Compare all three noise processing methods
noise_procs = c("AROMA+2P", "AROMA+2P+GMR", "AROMA+2P+DiCER")

# Use linear SVM
svm_kernel = "linear"

# Run theft's multivariable classifier on each ROI and save to an RDS object
# If the RDS object doesn't already exist, otherwise load it in
if (!file.exists(paste0(rdata_path, "UCLA_multivar_feature_ROI_res_svmLinear_in_sample_downsampled.Rds"))) {
  set.seed(127)
  feature_ROI_wise_SVM_in_sample_downsampled <- run_in_sample_ksvm_by_feature_and_ROI(rdata_path = rdata_path,
                                                                        svm_kernel = svm_kernel,
                                                                        noise_procs = noise_procs,
                                                                        use_inv_prob_weighting = FALSE,
                                                                        upsample_minority = FALSE,
                                                                        downsample_majority = TRUE) %>%
    dplyr::rename("accuracy" = "Accuracy",
                  "balanced_accuracy" = "Balanced_Accuracy")
  saveRDS(feature_ROI_wise_SVM_in_sample_downsampled, file=paste0(rdata_path, "UCLA_multivar_feature_ROI_res_svmLinear_in_sample_downsampled.Rds"))
} else {
  feature_ROI_wise_SVM_in_sample_downsampled <- readRDS(paste0(rdata_path, "UCLA_multivar_feature_ROI_res_svmLinear_in_sample_downsampled.Rds"))
}
```

```{r, fig.width=7, fig.height=6}
# Plot accuracy + balanced accuracy in histograms
# Control subject proportion is highlighted for accuracy
noise_procs = c("AROMA+2P", "AROMA+2P+GMR", "AROMA+2P+DiCER")
plot_class_acc_w_props(class_res = feature_ROI_wise_SVM_in_sample_downsampled,
                       cv = FALSE,
                       rdata_path = rdata_path,
                       noise_procs = noise_procs)
```
```{r, echo=F, eval=F}
# Save plot
ggsave("plots/In_Sample_Feature_Wise_linear_SVM_Multi_Region_Downsampled.png",
       width=7, height=6, units="in", dpi=300)
```

Alternatively, we can implement upsampling for the minority class (schizophrenia) such that the classes are perfectly balanced. This involves randomly sampling the schizophrenia subjects with replacement N times, where N is the number of control subjects (i.e. majority class).

This takes away the classifier's bias toward the majority class. The mean accuracy is right around 0.72, and the balanced accuracy is much higher -- with a mean of around 0.72 as well.

This indicates that minority class upsampling also mitigates the class imbalance issue and can be carried forward into 10-fold cross-validation linear SVM.

## Cross-validated SVM classification

### 10-fold cross-validated linear SVM

We can implement 10-fold cross-validation (CV) with the `caret` package.

```{r}
# Try three different noise processing methods
noise_procs = c("AROMA+2P", "AROMA+2P+GMR", "AROMA+2P+DiCER")

# Retain balanced accuracy in addition to raw accuracy for each ROI
use_balanced_accuracy <- TRUE

use_inv_prob_weighting = FALSE
downsample_majority = FALSE
upsample_minority = FALSE

# Run theft's multivariable classifier on each ROI and save to an RDS object
# If the RDS object doesn't already exist, otherwise load it in
if (!file.exists(paste0(rdata_path, "UCLA_feature_ROI_wise_multi_svmLinear_CV.Rds"))) {
  
   feature_ROI_wise_SVM_caret <- run_caret_multi_SVM_by_feature_and_ROI(rdata_path = rdata_path,
                                            use_inv_prob_weighting = use_inv_prob_weighting,
                                            upsample_minority = upsample_minority,
                                            downsample_majority = downsample_majority,
                                            noise_procs = noise_procs)  %>%
    dplyr::rename("accuracy" = "Accuracy",
                  "balanced_accuracy" = "Balanced_Accuracy")
   
  saveRDS(feature_ROI_wise_SVM_caret, file=paste0(rdata_path, "UCLA_feature_ROI_wise_multi_svmLinear_CV.Rds"))
} else {
  feature_ROI_wise_SVM_caret <- readRDS(paste0(rdata_path, "UCLA_feature_ROI_wise_multi_svmLinear_CV.Rds"))
}
```
```{r, fig.width=7, fig.height=6}
# Plot accuracy + balanced accuracy in histograms
# Control subject proportion is highlighted for accuracy
noise_procs = c("AROMA+2P", "AROMA+2P+GMR", "AROMA+2P+DiCER")
plot_class_acc_w_props(class_res = feature_ROI_wise_SVM_caret,
                       cv = TRUE,
                       rdata_path = rdata_path,
                       noise_procs = noise_procs)
```

```{r, echo=F, eval=F}
# Save plot
ggsave("plots/Caret_10CV_Feature_Wise_SVM_Multi_ROI_Feature_Combo.png",
       width=7, height=6, units="in", dpi=300)
```

As with in-sample SVM, the unweighted input samples are virtually all classified as control subjects across all 82 ROIs using the 10-fold cross-validation linear SVM with caret.

### 10-fold cross-validated linear SVM with inverse probability weighting

```{r}
# Try three different noise processing methods
noise_procs = c("AROMA+2P", "AROMA+2P+GMR", "AROMA+2P+DiCER")

# Retain balanced accuracy in addition to raw accuracy for each ROI
use_balanced_accuracy <- TRUE

# Implement inverse probability weighting
use_inv_prob_weighting = TRUE
upsample_minority = FALSE
downsample_majority = FALSE

# Run theft's multivariable classifier on each ROI and save to an RDS object
# If the RDS object doesn't already exist, otherwise load it in
if (!file.exists(paste0(rdata_path, "UCLA_feature_ROI_wise_multi_svmLinear_CV_inv_prob.Rds"))) {
  
   feature_ROI_wise_SVM_caret_inv_prob_df <- run_caret_multi_SVM_by_feature_and_ROI(rdata_path = rdata_path,
                                            use_inv_prob_weighting = use_inv_prob_weighting,
                                            upsample_minority = upsample_minority,
                                            downsample_majority = downsample_majority,
                                            noise_procs = noise_procs)  %>%
    dplyr::rename("accuracy" = "Accuracy",
                  "balanced_accuracy" = "Balanced_Accuracy")
   
  saveRDS(feature_ROI_wise_SVM_caret_inv_prob_df, file=paste0(rdata_path, "UCLA_feature_ROI_wise_multi_svmLinear_CV_inv_prob.Rds"))
} else {
  feature_ROI_wise_SVM_caret_inv_prob_df <- readRDS(paste0(rdata_path, "UCLA_feature_ROI_wise_multi_svmLinear_CV_inv_prob.Rds"))
}
```

```{r, fig.width=7, fig.height=6}
plot_class_acc_w_props(class_res = feature_ROI_wise_SVM_caret_inv_prob_df,
                       rdata_path = rdata_path,
                       noise_procs = noise_procs)
```
```{r, eval=F, echo=F}
# Save plot
ggsave("plots/Caret_10CV_Feature_Wise_SVM_Multi_ROI_Feature_Combo_Inv_Prob.png",
       width=7, height=6, units="in", dpi=300)
```

Surprisingly, incorporating inverse probability weighting has minimal impact when it comes to the ten-fold cross-validated SVM. Of note, the in-sample and cross-validated SVM were both run with kernlab::ksvm using default parameters.

### 10-fold cross-validated linear SVM with minority class upsampling

```{r}
# Try three different noise processing methods
noise_procs = c("AROMA+2P", "AROMA+2P+GMR", "AROMA+2P+DiCER")

# Retain balanced accuracy in addition to raw accuracy for each ROI
use_balanced_accuracy <- TRUE

# Implement inverse probability weighting
use_inv_prob_weighting = FALSE
upsample_minority = TRUE
downsample_majority = FALSE

# Run theft's multivariable classifier on each ROI and save to an RDS object
# If the RDS object doesn't already exist, otherwise load it in
if (!file.exists(paste0(rdata_path, "UCLA_feature_ROI_wise_multi_svmLinear_CV_upsampled.Rds"))) {
  
   feature_ROI_wise_SVM_caret_upsampled <- run_caret_multi_SVM_by_feature_and_ROI(rdata_path = rdata_path,
                                            use_inv_prob_weighting = use_inv_prob_weighting,
                                            upsample_minority = upsample_minority,
                                            downsample_majority = downsample_majority,
                                            noise_procs = noise_procs)  %>%
    dplyr::rename("accuracy" = "Accuracy",
                  "balanced_accuracy" = "Balanced_Accuracy")
   
  saveRDS(feature_ROI_wise_SVM_caret_upsampled, file=paste0(rdata_path, "UCLA_feature_ROI_wise_multi_svmLinear_CV_upsampled.Rds"))
} else {
  feature_ROI_wise_SVM_caret_upsampled <- readRDS(paste0(rdata_path, "UCLA_feature_ROI_wise_multi_svmLinear_CV_upsampled.Rds"))
}
```

```{r, fig.width=7, fig.height=6}
plot_class_acc_w_props(class_res = feature_ROI_wise_SVM_caret_upsampled,
                       rdata_path = rdata_path,
                       noise_procs = noise_procs)
```
```{r, eval=F, echo=F}
# Save plot
ggsave("plots/Caret_10CV_Feature_Wise_SVM_Multi_ROI_Feature_Combo_Upsampled.png",
       width=7, height=6, units="in", dpi=300)
```

After implementing sample reweighting with 10-fold cross-validation for SVM using caret, we see that the raw accuracy still has a mean of approximately 0.68 across all ROIs, although the balanced accuracy has a mean between 0.5-0.55.

To understand the significance of these values, we can generate a null distribution of classification accuracies and balanced accuracies using a model-free shuffle technique, modified from Trent's theft package.

### 10-fold cross-validated linear SVM with majority class downsampling

```{r}
# Try three different noise processing methods
noise_procs = c("AROMA+2P", "AROMA+2P+GMR", "AROMA+2P+DiCER")

# Retain balanced accuracy in addition to raw accuracy for each ROI
use_balanced_accuracy <- TRUE

# Implement inverse probability weighting
use_inv_prob_weighting = FALSE
upsample_minority = FALSE
downsample_majority = TRUE

# Run theft's multivariable classifier on each ROI and save to an RDS object
# If the RDS object doesn't already exist, otherwise load it in
if (!file.exists(paste0(rdata_path, "UCLA_feature_ROI_wise_multi_svmLinear_CV_downsampled.Rds"))) {
  
   feature_ROI_wise_SVM_caret_downsampled <- run_caret_multi_SVM_by_feature_and_ROI(rdata_path = rdata_path,
                                            use_inv_prob_weighting = use_inv_prob_weighting,
                                            upsample_minority = upsample_minority,
                                            downsample_majority = downsample_majority,
                                            noise_procs = noise_procs)  %>%
    dplyr::rename("accuracy" = "Accuracy",
                  "balanced_accuracy" = "Balanced_Accuracy")
   
  saveRDS(feature_ROI_wise_SVM_caret_downsampled, file=paste0(rdata_path, "UCLA_feature_ROI_wise_multi_svmLinear_CV_downsampled.Rds"))
} else {
  feature_ROI_wise_SVM_caret_downsampled <- readRDS(paste0(rdata_path, "UCLA_feature_ROI_wise_multi_svmLinear_CV_downsampled.Rds"))
}
```

```{r, fig.width=7, fig.height=6}
plot_class_acc_w_props(class_res = feature_ROI_wise_SVM_caret_downsampled,
                       rdata_path = rdata_path,
                       noise_procs = noise_procs)
```
```{r, eval=F, echo=F}
# Save plot
ggsave("plots/Caret_10CV_Feature_Wise_SVM_Multi_ROI_Feature_Combo_Downsampled.png",
       width=7, height=6, units="in", dpi=300)
```

After implementing sample reweighting with 10-fold cross-validation for SVM using caret, we see that the raw accuracy still has a mean of approximately 0.68 across all ROIs, although the balanced accuracy has a mean between 0.5-0.55.

To understand the significance of these values, we can generate a null distribution of classification accuracies and balanced accuracies using a model-free shuffle technique, modified from Trent's theft package.



## Null models

### Generating null distributions from model-free shuffles

```{r}
# Try three different noise processing methods
noise_procs = c("AROMA+2P", "AROMA+2P+GMR", "AROMA+2P+DiCER")

# One without minority upsampling
if (!file.exists(paste0(rdata_path, "Null_Model_Free_Shuffles.Rds"))) {
  set.seed(127) 
  model_free_shuffle_null_res <- run_model_free_n_shuffles(num_shuffles = 1000000,
                                                    rdata_path = rdata_path,
                                                    noise_procs = noise_procs)
  saveRDS(model_free_shuffle_null_res, file = paste0(rdata_path, "Null_Model_Free_Shuffles.Rds"))
} else {
  model_free_shuffle_null_res <- readRDS(paste0(rdata_path, "Null_Model_Free_Shuffles.Rds"))
}

# One with minority upsampling
if (!file.exists(paste0(rdata_path, "Null_Model_Free_Shuffles_Upsampled.Rds"))) {
  set.seed(127) 
  model_free_shuffle_null_upsampled <- run_model_free_n_shuffles(num_shuffles = 1000000,
                                                    rdata_path = rdata_path,
                                                    noise_procs = noise_procs,
                                                    use_upsampling = TRUE)
  saveRDS(model_free_shuffle_null_upsampled, file = paste0(rdata_path, "Null_Model_Free_Shuffles_Upsampled.Rds"))
} else {
  model_free_shuffle_null_upsampled <- readRDS(paste0(rdata_path, "Null_Model_Free_Shuffles_Upsampled.Rds"))
}
```

### Deriving p-values using model-free shuffle null distributions

```{r}
if (!file.exists(paste0(rdata_path, "UCLA_feature_ROI_wise_SVM_caret_upsampled_pvals.Rds"))) {
  feature_ROI_wise_SVM_caret_upsampled_pvals <- calc_empirical_nulls(class_res = feature_ROI_wise_SVM_caret_upsampled,
                                                              null_data = model_free_shuffle_null_upsampled)
  saveRDS(feature_ROI_wise_SVM_caret_upsampled_pvals, file=paste0(rdata_path, "UCLA_feature_ROI_wise_SVM_caret_upsampled_pvals.Rds"))
  
} else {
  feature_ROI_wise_SVM_caret_upsampled_pvals <- readRDS(paste0(rdata_path, "UCLA_feature_ROI_wise_SVM_caret_upsampled_pvals.Rds"))
}


feature_ROI_wise_SVM_caret_upsampled_plabs <- feature_ROI_wise_SVM_caret_upsampled_pvals %>%
  mutate(acc_p = scales::scientific(acc_p, digits = 3),
         bal_acc_p = scales::scientific(bal_acc_p, digits = 3),
         acc_p_adj = scales::scientific(acc_p_adj, digits = 3),
         bal_acc_p_adj = scales::scientific(bal_acc_p_adj, digits = 3))
```

```{r, fig.width=7, fig.height=6}
null_hist_data <- feature_ROI_wise_SVM_caret_upsampled %>%
  mutate(Type = "main") %>%
  plyr::rbind.fill(., model_free_shuffle_null_upsampled) %>%
  pivot_longer(cols=c(accuracy, balanced_accuracy),
               names_to = "Metric",
               values_to = "Values") %>%
  mutate(Metric = stringr::str_to_title(str_replace_all(Metric, "_", " "))) %>%
  mutate(Noise_Proc = factor(Noise_Proc, levels = c("AROMA+2P",
                                                    "AROMA+2P+GMR",
                                                    "AROMA+2P+DiCER"))) 

ggplot() +
  geom_histogram(data = subset(null_hist_data, Type=="null"),
                 aes(x = Values, y=0.5*..density..), 
                 bins = 50,
                 alpha=0.6, position="identity",
                 fill = "gray70") +
  geom_vline(data = subset(null_hist_data, Type=="main"),
             aes(xintercept = Values), color="red") +
  facet_grid(Noise_Proc ~ Metric, switch="y", scales="free_x") +
  xlab("Value from n=1,000,000 Model-Free Shuffles") +
  ylab("Scaled Density") +
  labs(fill = "Distribution") +
  geom_text(data = feature_ROI_wise_SVM_caret_upsampled_plabs,
            aes(label = paste0("P = ", acc_p, "\nBH-FDR = ", acc_p_adj)), 
            x = 0.75, y = 7) +
  theme(strip.text.y.left = element_text(angle=0),
        strip.placement = "outside",
        legend.position = "bottom",
        legend.direction = "horizontal")

ggsave("plots/Main_vs_Null_Acc_BAcc_Caret_10FCV_Linear_SVM_Feature_ROI_Wise.png",
       width=7, height=6, units="in", dpi=300)
```

```{r, eval=F, echo=F}
# Save plot
ggsave("plots/Main_vs_Null_Acc_BAcc_Caret_10FCV_Linear_SVM_Feature_ROI_Wise.png",
       width=7, height=6, units="in", dpi=300)
```

The model-free shuffles method is borrowed from Trent's implementation in theft. With this method, the input class labels (Schz or Control) are randomly shuffled N times, and for each iteration, the classification accuracy and balanced accuracy is calculated. This yields a null distribution of accuracies and balanced accuracies, circumventing the need for running any classification algorithms across iterations.

Here, I've run 1,000,000 iterations of the model-free shuffle, generating 1,000,000 null values for Accuracy and Balanced Accuracy, respectively. Since this method is independent of brain region, the same null distribution can be used to compare with each brain region separately.

I've plotted the distribution of null accuracies (coral) alongside the actual accuracies (teal) for the 82 ROIs on the left. 

We can empirically derive p-values for both metrics to see if the classification accuracy/balanced accuracy is significantly greater than that in the null distribution.


## Summary statistics

### Accuracy summary

```{r}
left_join(feature_ROI_wise_SVM_caret_upsampled, feature_ROI_wise_SVM_caret_upsampled_plabs) %>%
  dplyr::select(Noise_Proc, accuracy, acc_p, acc_p_adj) %>%
  mutate(accuracy = 100*round(accuracy, 4)) 
```



