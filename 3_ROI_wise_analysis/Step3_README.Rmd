---
title: "Step 3: ROI-Wise catch22 Feature Analysis"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=F, message=F)
```

### Source functions
```{r}
source("../helper_functions/in_sample_SVM_code.R")
rdata_path <- "D:/Virtual_Machines/Shared_Folder/PhD_work/data/scz/UCLA/Rdata/"
set.seed(127)
```


## In-sample SVM classification

### Simple in-sample linear SVM

We will start with a simple linear SVM classifier using all 22 features.

```{r}
# Compare all three noise processing methods
noise_procs = c("AROMA+2P", "AROMA+2P+GMR", "AROMA+2P+DiCER")

# SVM param df
svm_df <- data.frame(test_packages = c("e1071", "kernlab"),
                     kernels = c("linear", "vanilladot"))

# Iterate over SVM params
for (i in 1:nrow(svm_df)) {
  test_package = svm_df$test_packages[i]
  kernel = svm_df$kernels[i]
  
  # Run in-sample SVM using given package + kernel
  # If the RDS object doesn't already exist, otherwise load it in
  if (!file.exists(paste0(rdata_path, "multivar_ROI_res_svmLinear_in_sample_", test_package, ".Rds"))) {
    region_wise_SVM_in_sample <- run_in_sample_svm_by_input_var(rdata_path = rdata_path,
                                                                svm_kernel = kernel,
                                                                test_package = test_package,
                                                                grouping_var = "Brain_Region",
                                                                svm_feature_var = "Feature",
                                                                noise_procs = noise_procs,
                                                                use_inv_prob_weighting = FALSE) %>%
      dplyr::rename("accuracy" = "Accuracy",
                    "balanced_accuracy" = "Balanced_Accuracy")
    saveRDS(region_wise_SVM_in_sample, file=paste0(rdata_path, "multivar_ROI_res_svmLinear_in_sample_", test_package, ".Rds"))
  } else {
    region_wise_SVM_in_sample <- readRDS(paste0(rdata_path, "multivar_ROI_res_svmLinear_in_sample_", test_package, ".Rds"))
  }
}
```

Let's compare e1071 with kernlab. 

e1071:

```{r, fig.width=7, fig.height=6}
# Plot accuracy + balanced accuracy in histograms
# Control subject proportion is highlighted for accuracy
e1071_region_wise_SVM_in_sample <- readRDS(paste0(rdata_path, "multivar_ROI_res_svmLinear_in_sample_e1071.Rds"))

noise_procs = c("AROMA+2P", "AROMA+2P+GMR", "AROMA+2P+DiCER")
plot_class_acc_w_props(class_res = e1071_region_wise_SVM_in_sample,
                       group_var = "Brain_Region",
                       cv = FALSE,
                       rdata_path = rdata_path,
                       noise_procs = noise_procs)
```

```{r, eval=F, echo=F}
# Save plot
ggsave("plots/In_Sample_Region_Wise_linear_SVM_Multi_Feature_e1071.png",
       width=7, height=6, units="in", dpi=300)
```

kernlab:

```{r, fig.width=7, fig.height=6}
# Plot accuracy + balanced accuracy in histograms
# Control subject proportion is highlighted for accuracy
kernlab_region_wise_SVM_in_sample <- readRDS(paste0(rdata_path, "multivar_ROI_res_svmLinear_in_sample_kernlab.Rds"))

noise_procs = c("AROMA+2P", "AROMA+2P+GMR", "AROMA+2P+DiCER")
plot_class_acc_w_props(class_res = kernlab_region_wise_SVM_in_sample,
                       group_var = "Brain_Region",
                       cv = FALSE,
                       rdata_path = rdata_path,
                       noise_procs = noise_procs)
```

```{r, eval=F, echo=F}
# Save plot
ggsave("plots/In_Sample_Region_Wise_linear_SVM_Multi_Feature_kernlab.png",
       width=7, height=6, units="in", dpi=300)
```

The figure on the left shows the results from running theft's multivariable classifier using all 22 catch22 features with svmLinear (kernlab) in caret.

The dashed line shows the proportion of control subjects in the population for Accuracy (left half).

Clearly, the classification algorithm is biased for the majority of ROIs to classify all samples as controls in order to achieve a raw accuracy of ~0.7. Similarly, the balanced accuracy is centered around 0.35.

This highlights the need to mitigate the class imbalances, either with inverse probability sample weighting or minority class upsampling.


### In-sample linear SVM with inverse probability weighting

We can run linear SVM with the `kernlab` package to directly test sample reweighting with in-sample accuracy and balanced accuracy. 

```{r}
# Compare all three noise processing methods
noise_procs = c("AROMA+2P", "AROMA+2P+GMR", "AROMA+2P+DiCER")

# SVM param df
svm_df <- data.frame(test_packages = c("e1071", "kernlab"),
                     kernels = c("linear", "vanilladot"))

# Iterate over SVM params
for (i in 1:nrow(svm_df)) {
  test_package = svm_df$test_packages[i]
  kernel = svm_df$kernels[i]
  
  # Run in-sample SVM using given package + kernel
  # If the RDS object doesn't already exist, otherwise load it in
  if (!file.exists(paste0(rdata_path, "multivar_ROI_res_svmLinear_in_sample_inv_prob_", test_package, ".Rds"))) {
    region_wise_SVM_in_sample_inv_prob <- run_in_sample_svm_by_input_var(rdata_path = rdata_path,
                                                                svm_kernel = kernel,
                                                                test_package = test_package,
                                                                grouping_var = "Brain_Region",
                                                                svm_feature_var = "Feature",
                                                                noise_procs = noise_procs,
                                                                use_inv_prob_weighting = TRUE) %>%
      dplyr::rename("accuracy" = "Accuracy",
                    "balanced_accuracy" = "Balanced_Accuracy")
    saveRDS(region_wise_SVM_in_sample_inv_prob, file=paste0(rdata_path, "multivar_ROI_res_svmLinear_in_sample_inv_prob_", test_package, ".Rds"))
  } 
}


```


Let's compare e1071 with kernlab. 

e1071:
```{r, fig.width=7, fig.height=6}
# Plot accuracy + balanced accuracy in histograms
# Control subject proportion is highlighted for accuracy
e1071_region_wise_SVM_in_sample_inv_prob <- readRDS(paste0(rdata_path, "multivar_ROI_res_svmLinear_in_sample_inv_prob_e1071.Rds"))

noise_procs = c("AROMA+2P", "AROMA+2P+GMR", "AROMA+2P+DiCER")
plot_class_acc_w_props(class_res = e1071_region_wise_SVM_in_sample_inv_prob,
                       group_var = "Brain_Region",
                       cv = FALSE,
                       rdata_path = rdata_path,
                       noise_procs = noise_procs)
```
```{r, echo=F, eval=F}
# Save plot
ggsave("plots/In_Sample_Region_Wise_SVM_Multi_Feature_Inv_Prob_e1071.png",
       width=7, height=6, units="in", dpi=300)
```

kernlab:
```{r, fig.width=7, fig.height=6}
# Plot accuracy + balanced accuracy in histograms
# Control subject proportion is highlighted for accuracy
kernlab_region_wise_SVM_in_sample_inv_prob <- readRDS(paste0(rdata_path, "multivar_ROI_res_svmLinear_in_sample_inv_prob_kernlab.Rds"))

noise_procs = c("AROMA+2P", "AROMA+2P+GMR", "AROMA+2P+DiCER")
plot_class_acc_w_props(class_res = kernlab_region_wise_SVM_in_sample_inv_prob,
                       group_var = "Brain_Region",
                       cv = FALSE,
                       rdata_path = rdata_path,
                       noise_procs = noise_procs)
```
```{r, echo=F, eval=F}
# Save plot
ggsave("plots/In_Sample_Region_Wise_SVM_Multi_Feature_Inv_Prob_kernlab.png",
       width=7, height=6, units="in", dpi=300)
```

By assigning each subject a weight equivalent to the inverse proportion of that subject's diagnosis, the linear SVM places a higher cost on incorrectly classifying schizophrenia subjects as controls. 

This shifts the raw accuracy down to a mean of around 0.68 across the three noise-processing methods, but the balanced accuracy increases to have an average of around 0.68 also -- compared with almost exclusively values of 0.35 previously.

This indicates that inverse probability reweighting mitigates the class imbalance issue and can be carried forward into 10-fold cross-validation linear SVM.




## Cross-validated SVM classification

### 10-fold cross-validated linear SVM

We can implement 10-fold cross-validation (CV) with the `caret` package.

```{r}
# Noise processing methods
noise_procs <- c("AROMA+2P", "AROMA+2P+GMR", "AROMA+2P+DiCER")

# SVM param df
svm_df <- data.frame(test_packages = c("e1071", "kernlab"),
                     kernels = c("linear", "vanilladot"))

# Iterate over SVM params
for (i in 1:nrow(svm_df)) {
  test_package = svm_df$test_packages[i]
  kernel = svm_df$kernels[i]
  
  # Run in-sample SVM using given package + kernel
  # If the RDS object doesn't already exist, otherwise load it in
  if (!file.exists(paste0(rdata_path, "multivar_ROI_res_svmLinear_CV_", test_package, ".Rds"))) {
    region_wise_SVM_in_sample <- run_cv_svm_by_input_var(rdata_path = rdata_path,
                                                         test_package = test_package,
                                                         svm_kernel = kernel,
                                                         grouping_var = "Brain_Region",
                                                         svm_feature_var = "Feature",
                                                         use_inv_prob_weighting = FALSE,
                                                         noise_procs = noise_procs)  %>%
      dplyr::rename("accuracy" = "Accuracy",
                    "balanced_accuracy" = "Balanced_Accuracy")
    saveRDS(region_wise_SVM_in_sample, file=paste0(rdata_path, "multivar_ROI_res_svmLinear_CV_", test_package, ".Rds"))
  }
}
```

Let's compare e1071 with kernlab. 

e1071:

```{r, fig.width=7, fig.height=6}
# Plot accuracy + balanced accuracy in histograms
# Control subject proportion is highlighted for accuracy
e1071_region_wise_SVM_CV <- readRDS(paste0(rdata_path, "multivar_ROI_res_svmLinear_CV_e1071.Rds"))

noise_procs = c("AROMA+2P", "AROMA+2P+GMR", "AROMA+2P+DiCER")
plot_class_acc_w_props(class_res = e1071_region_wise_SVM_CV,
                       group_var = "Brain_Region",
                       cv = TRUE,
                       rdata_path = rdata_path,
                       noise_procs = noise_procs)
```

```{r, eval=F, echo=F}
# Save plot
ggsave("plots/Region_Wise_linear_SVM_CV_Multi_Feature_e1071.png",
       width=7, height=6, units="in", dpi=300)
```

kernlab:

```{r, fig.width=7, fig.height=6}
# Plot accuracy + balanced accuracy in histograms
# Control subject proportion is highlighted for accuracy
kernlab_region_wise_SVM_CV <- readRDS(paste0(rdata_path, "multivar_ROI_res_svmLinear_CV_kernlab.Rds"))

noise_procs = c("AROMA+2P", "AROMA+2P+GMR", "AROMA+2P+DiCER")
plot_class_acc_w_props(class_res = kernlab_region_wise_SVM_CV,
                       group_var = "Brain_Region",
                       cv = TRUE,
                       rdata_path = rdata_path,
                       noise_procs = noise_procs)
```

```{r, eval=F, echo=F}
# Save plot
ggsave("plots/Region_Wise_linear_SVM_CV_Multi_Feature_kernlab.png",
       width=7, height=6, units="in", dpi=300)
```


As with in-sample SVM, the unweighted input samples are virtually all classified as control subjects across all 82 ROIs using the 10-fold cross-validation linear SVM with caret.

### 10-fold cross-validated linear SVM with inverse probability weighting

```{r}
# Noise processing methods
noise_procs <- c("AROMA+2P", "AROMA+2P+GMR", "AROMA+2P+DiCER")

# SVM param df
svm_df <- data.frame(test_packages = c("e1071", "kernlab"),
                     kernels = c("linear", "vanilladot"))

# Iterate over SVM params
for (i in 1:nrow(svm_df)) {
  test_package = svm_df$test_packages[i]
  kernel = svm_df$kernels[i]
  
  # Run in-sample SVM using given package + kernel
  # If the RDS object doesn't already exist, otherwise load it in
  if (!file.exists(paste0(rdata_path, "multivar_ROI_res_svmLinear_CV_", test_package, "_inv_prob.Rds"))) {
    region_wise_SVM_in_sample_inv_prob <- run_cv_svm_by_input_var(rdata_path = rdata_path,
                                                                  svm_kernel = kernel,
                                                                  test_package = test_package,
                                                                  grouping_var = "Brain_Region",
                                                                  svm_feature_var = "Feature",
                                                                  use_inv_prob_weighting = TRUE,
                                                                  noise_procs = noise_procs)  %>%
      dplyr::rename("accuracy" = "Accuracy",
                    "balanced_accuracy" = "Balanced_Accuracy")
    saveRDS(region_wise_SVM_in_sample_inv_prob, file=paste0(rdata_path, "multivar_ROI_res_svmLinear_CV_", test_package, "_inv_prob.Rds"))
  }
}
```


Let's compare e1071 with kernlab. 

e1071:

```{r, fig.width=7, fig.height=6}
# Plot accuracy + balanced accuracy in histograms
# Control subject proportion is highlighted for accuracy
e1071_region_wise_SVM_CV_inv_prob <- readRDS(paste0(rdata_path, "multivar_ROI_res_svmLinear_CV_e1071_inv_prob.Rds"))

noise_procs = c("AROMA+2P", "AROMA+2P+GMR", "AROMA+2P+DiCER")
plot_class_acc_w_props(class_res = e1071_region_wise_SVM_CV_inv_prob,
                       group_var = "Brain_Region",
                       cv = TRUE,
                       rdata_path = rdata_path,
                       noise_procs = noise_procs)
```

```{r, eval=F, echo=F}
# Save plot
ggsave("plots/Region_Wise_linear_SVM_CV_Multi_Feature_Inv_Prob_e1071.png",
       width=7, height=6, units="in", dpi=300)
```

kernlab:

```{r, fig.width=7, fig.height=6}
# Plot accuracy + balanced accuracy in histograms
# Control subject proportion is highlighted for accuracy
kernlab_region_wise_SVM_CV_inv_prob <- readRDS(paste0(rdata_path, "multivar_ROI_res_svmLinear_CV_kernlab_inv_prob.Rds"))

noise_procs = c("AROMA+2P", "AROMA+2P+GMR", "AROMA+2P+DiCER")
plot_class_acc_w_props(class_res = kernlab_region_wise_SVM_CV_inv_prob,
                       cv = TRUE,
                       rdata_path = rdata_path,
                       noise_procs = noise_procs)
```

```{r, eval=F, echo=F}
# Save plot
ggsave("plots/Region_Wise_linear_SVM_CV_Multi_Feature_Inv_Prob_kernlab.png",
       width=7, height=6, units="in", dpi=300)
```


Surprisingly, incorporating inverse probability weighting has minimal impact when it comes to the ten-fold cross-validated SVM. Of note, the in-sample and cross-validated SVM were both run with kernlab::ksvm using default parameters.

## Null distribution
### Generating null distributions from model-free shuffles

```{r}
# Try three different noise processing methods
noise_procs = c("AROMA+2P", "AROMA+2P+GMR", "AROMA+2P+DiCER")

# One without minority upsampling
if (!file.exists(paste0(rdata_path, "Null_Model_Free_Shuffles.Rds"))) {
  set.seed(127) 
  model_free_shuffle_null_res <- run_model_free_n_shuffles(num_shuffles = 1000000,
                                                    rdata_path = rdata_path,
                                                    noise_procs = noise_procs)
  saveRDS(model_free_shuffle_null_res, file = paste0(rdata_path, "Null_Model_Free_Shuffles.Rds"))
} else {
  model_free_shuffle_null_res <- readRDS(paste0(rdata_path, "Null_Model_Free_Shuffles.Rds"))
}
```


```{r, fig.width=7, fig.height=6}
e1071_region_wise_SVM_CV <- readRDS(paste0(rdata_path, "multivar_ROI_res_svmLinear_CV_e1071.Rds"))

e1071_region_wise_SVM_CV %>%
  dplyr::select(grouping_var, Noise_Proc, accuracy, balanced_accuracy) %>%
  mutate(Type = "main") %>%
  plyr::rbind.fill(., model_free_shuffle_null_res) %>%
  pivot_longer(cols=c(accuracy, balanced_accuracy),
               names_to = "Metric",
               values_to = "Values") %>%
  mutate(Metric = stringr::str_to_title(str_replace_all(Metric, "_", " "))) %>%
  mutate(Noise_Proc = factor(Noise_Proc, levels = c("AROMA+2P",
                                                    "AROMA+2P+GMR",
                                                    "AROMA+2P+DiCER"))) %>%
  ggplot(data=., mapping=aes(x=Values)) +
  geom_histogram(aes(fill = Type, y=0.5*..density..), 
                 bins = 50,
                 alpha=0.6, position="identity") +
  facet_grid(Noise_Proc ~ Metric, switch="y", scales="free_x") +
  xlab("Value from n=1,000,000 Model-Free Shuffles") +
  ylab("Scaled Density") +
  labs(fill = "Distribution") +
  theme(strip.text.y.left = element_text(angle=0),
        strip.placement = "outside",
        legend.position = "bottom",
        legend.direction = "horizontal") 
```

```{r, eval=F, echo=F}
# Save plot
ggsave("plots/Main_vs_Null_Acc_BAcc_10FCV_ROI_Wise_Linear_SVM.png",
       width=7, height=6, units="in", dpi=300)
```

The model-free shuffles method is borrowed from Trent's implementation in theft. With this method, the input class labels (Schz or Control) are randomly shuffled N times, and for each iteration, the classification accuracy and balanced accuracy is calculated. This yields a null distribution of accuracies and balanced accuracies, circumventing the need for running any classification algorithms across iterations.

Here, I've run 1,000,000 iterations of the model-free shuffle, generating 1,000,000 null values for Accuracy and Balanced Accuracy, respectively. Since this method is independent of brain region, the same null distribution can be used to compare with each brain region separately.

I've plotted the distribution of null accuracies (coral) alongside the actual accuracies (teal) for the 82 ROIs on the left. We can also compare the results with those from using inverse probability weighting:

```{r, fig.width=7, fig.height=6}
e1071_region_wise_SVM_CV_inv_prob %>%
  dplyr::select(grouping_var, Noise_Proc, accuracy, balanced_accuracy) %>%
  mutate(Type = "main") %>%
  plyr::rbind.fill(., model_free_shuffle_null_res) %>%
  pivot_longer(cols=c(accuracy, balanced_accuracy),
               names_to = "Metric",
               values_to = "Values") %>%
  mutate(Metric = stringr::str_to_title(str_replace_all(Metric, "_", " "))) %>%
  mutate(Noise_Proc = factor(Noise_Proc, levels = c("AROMA+2P",
                                                    "AROMA+2P+GMR",
                                                    "AROMA+2P+DiCER"))) %>%
  ggplot(data=., mapping=aes(x=Values)) +
  geom_histogram(aes(fill = Type, y=0.5*..density..), 
                 bins = 50,
                 alpha=0.6, position="identity") +
  facet_grid(Noise_Proc ~ Metric, switch="y", scales="free_x") +
  xlab("Value from n=1,000,000 Model-Free Shuffles") +
  ylab("Scaled Density") +
  labs(fill = "Distribution") +
  theme(strip.text.y.left = element_text(angle=0),
        strip.placement = "outside",
        legend.position = "bottom",
        legend.direction = "horizontal") 
```

```{r, eval=F, echo=F}
# Save plot
ggsave("plots/Main_vs_Null_Acc_BAcc_10FCV_ROI_Wise_Linear_SVM_inv_prob.png",
       width=7, height=6, units="in", dpi=300)
```


We can empirically derive p-values for both metrics to see if the classification accuracy/balanced accuracy is significantly greater than that in the null distribution.

### Deriving p-values using model-free shuffle null distributions

```{r}
if (!file.exists(paste0(rdata_path, "ROI_wise_10FCV_linear_SVM_e1071_inv_prob_pvals.Rds"))) {
  e1071_region_wise_SVM_CV_inv_prob_pvals <- calc_empirical_nulls(class_res = e1071_region_wise_SVM_CV_inv_prob,
                                                              null_data = model_free_shuffle_null_res,
                                                              grouping_var = "Brain_Region")
  saveRDS(e1071_region_wise_SVM_CV_inv_prob_pvals, file=paste0(rdata_path, "ROI_wise_10FCV_linear_SVM_e1071_inv_prob_pvals.Rds"))
} else {
  e1071_region_wise_SVM_CV_inv_prob_pvals <- readRDS(paste0(rdata_path, "ROI_wise_10FCV_linear_SVM_e1071_inv_prob_pvals.Rds"))
}

e1071_region_wise_SVM_CV_inv_prob_plabs <- e1071_region_wise_SVM_CV_inv_prob_pvals %>%
  mutate(acc_p = scales::scientific(acc_p, digits = 3),
         bal_acc_p = scales::scientific(bal_acc_p, digits = 3),
         acc_p_adj = scales::scientific(acc_p_adj, digits = 3),
         bal_acc_p_adj = scales::scientific(bal_acc_p_adj, digits = 3))
```


Let's zoom in on AROMA+2P+DiCER and pick the five brain regions with the highest cross-validated accuracy:
```{r, fig.width=10, fig.height=8}
brain_regions <- e1071_region_wise_SVM_CV_inv_prob_plabs %>%
  filter(Noise_Proc == "AROMA+2P") %>%
  arrange(desc(accuracy)) %>%
  top_n(5, accuracy) %>%
  pull(grouping_var)

e1071_region_wise_SVM_CV_inv_prob_pvals %>%
  filter(Noise_Proc == "AROMA+2P",
         grouping_var %in% brain_regions) %>%
  mutate(grouping_var = factor(grouping_var, levels = brain_regions)) %>%
  ggplot(data=.) +
  geom_histogram(data = model_free_shuffle_null_res %>% 
                   dplyr::filter(Noise_Proc == "AROMA+2P"),
                 aes(x=accuracy, y=0.5*..density..),
                 fill = "gray70", bins=50) +
  ggtitle("Main and Model-Free Shuffle Null Accuracy\nfor top AROMA+2 Brain Regions") +
  geom_vline(aes(xintercept = accuracy), color = "red") +
  facet_wrap(grouping_var ~ ., scales="free_y", nrow = 2) +
  ylab("Scaled Density of Null Iterations") +
  geom_text(data = e1071_region_wise_SVM_CV_inv_prob_plabs %>%
              filter(Noise_Proc == "AROMA+2P", grouping_var %in% brain_regions) %>%
              mutate(grouping_var = factor(grouping_var, levels = brain_regions)),
            aes(label = paste0("P = ", acc_p, "\nBH-FDR = ", acc_p_adj)), 
            x = 0.67, y = 10.5) +
  theme(plot.title = element_text(hjust=0.5))
```
```{r, echo=F, eval=F}
ggsave("plots/Main_vs_Null_Acc_AROMA_2P_Top5.png", width=10, 
       height=6, units="in", dpi=300)
```

This panel panel shows the five brain regions with the highest raw accuracy from multi-feature 10-fold CV linear SVM. The red bar indicates the real accuracy while the gray bars depict the null distribution (derived from model-free shuffles as shown above). 

Even after adjusting for multiple comparisons (82 ROIs x 3 Noise-processing methods) using Benjamini-Hochberg False Discovery Rate (BH-FDR), the accuracy values for these five regions are still significant (p < 1.00e-06, FDR < 1.00e-05).

We can do the same for balanced accuracy:
```{r, fig.width=10, fig.height=8}
brain_regions <- e1071_region_wise_SVM_CV_inv_prob_plabs %>%
  filter(Noise_Proc == "AROMA+2P") %>%
  arrange(desc(balanced_accuracy)) %>%
  top_n(5, balanced_accuracy) %>%
  pull(grouping_var)

e1071_region_wise_SVM_CV_inv_prob_pvals %>%
  filter(Noise_Proc == "AROMA+2P",
         grouping_var %in% brain_regions) %>%
  mutate(grouping_var = factor(grouping_var, levels = brain_regions)) %>%
  ggplot(data=.) +
  geom_histogram(data = model_free_shuffle_null_res %>% 
                   dplyr::filter(Noise_Proc == "AROMA+2P"),
                 aes(x=balanced_accuracy, y=0.5*..density..),
                 fill = "gray70", bins=50) +
  ggtitle("Main and Model-Free Shuffle Null Balanced Accuracy\nfor top AROMA+2P Brain Regions") +
  geom_vline(aes(xintercept = balanced_accuracy), color = "red") +
  facet_wrap(grouping_var ~ ., scales="free_y", nrow = 2) +
  ylab("Scaled Density of Null Iterations") +
  geom_text(data = e1071_region_wise_SVM_CV_inv_prob_plabs %>%
              filter(Noise_Proc == "AROMA+2P", grouping_var %in% brain_regions) %>%
              mutate(grouping_var = factor(grouping_var, levels = brain_regions)),
            aes(label = paste0("P = ", bal_acc_p, "\nBH-FDR = ", bal_acc_p_adj)), 
            x = 0.6, y = 9) +
  theme(plot.title = element_text(hjust=0.5))
```
```{r, echo=F, eval=F}
ggsave("plots/Main_vs_Null_BalAcc_AROMA_2P_Top5.png", width=10, 
       height=6, units="in", dpi=300)
```

This panel panel shows the five brain regions with the highest balanced accuracy from multi-feature 10-fold CV linear SVM. The red bar indicates the real accuracy while the gray bars depict the null distribution (derived from model-free shuffles as shown above). 

Even after adjusting for multiple comparisons (82 ROIs x 3 Noise-processing methods) using Benjamini-Hochberg False Discovery Rate (BH-FDR), the accuracy values for these five regions are still significant (p = 0, BH-FDR = 0).

```{r}
e1071_region_wise_SVM_CV_inv_prob_pvals %>%
  mutate(Noise_Proc = factor(Noise_Proc, levels = noise_procs)) %>%
  group_by(Noise_Proc) %>%
  summarise(num_sig_acc = sum(acc_p < 0.05),
            num_sig_acc_fdr = sum(acc_p_adj < 0.05),
            num_sig_bacc = sum(bal_acc_p < 0.05),
            num_sig_bacc_fdr = sum(bal_acc_p_adj < 0.05))
```

This table summarises the number of ROIs for which raw accuracy or balanced accuracy is significantly greater than the upsampled model-free shuffle null distribution, both before and after adjusting for multiple comparisons with BH-FDR.  


