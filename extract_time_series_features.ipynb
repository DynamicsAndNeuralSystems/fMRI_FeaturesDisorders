{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting time-series features from resting-state fMRI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "from pyarrow import feather\n",
    "from nilearn.image import math_img, resample_img, index_img, threshold_img\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the data\n",
    "\n",
    "We will start by computing the mean framewise displacement (FD) for each participant so that we know which participant(s) to exclude from further analysis.\n",
    "FD is computed using the method from [Power et al. (2012)](https://doi.org/10.1016/j.neuroimage.2011.10.018) and we apply the 'lenient' threshold described in [Parkes et al. (2018)](https://doi.org/10.1016/j.neuroimage.2017.12.073)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2023 The MathWorks, Inc.\n",
      "                  R2023a (9.14.0.2206163) 64-bit (maci64)\n",
      "                             February 22, 2023\n",
      "\n",
      " \n",
      "To get started, type doc.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2023 The MathWorks, Inc.\n",
      "                  R2023a (9.14.0.2206163) 64-bit (maci64)\n",
      "                             February 22, 2023\n",
      "\n",
      " \n",
      "To get started, type doc.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Define the path to the data\n",
    "github_path=$(echo $(pwd) | tr -d ' ')\n",
    "\n",
    "# Run the calc_FD.m script -- note that you might need to update your matlab path here\n",
    "cd code/movement_analysis\n",
    "\n",
    "# UCLA CNP\n",
    "matlab -nodisplay -singleCompThread -r \"calc_FD UCLA_CNP $github_path $github_path/data/movement_data/UCLA_CNP/; exit\"\n",
    "\n",
    "# ABIDE\n",
    "matlab -nodisplay -singleCompThread -r \"calc_FD ABIDE $github_path $github_path/data/movement_data/ABIDE/; exit\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "FD data should be organized in a `.txt` file with two columns (comma-delimited): one for the `Sample_ID` and one for the `Mean_FD_Power`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UCLA_CNP_mean_FD = pd.read_table('data/movement_data/UCLA_CNP/UCLA_CNP_Mean_FD_Power.txt', delimiter=',', header=None).rename(columns={0: 'Sample_ID', 1: 'Mean_FD_Power'})\n",
    "ABIDE_mean_FD = pd.read_table('data/movement_data/ABIDE/ABIDE_Mean_FD_Power.txt', delimiter=',', header=None).rename(columns={0: 'Sample_ID', 1: 'Mean_FD_Power'})\n",
    "\n",
    "# Print the first five rows of the UCLA CNP Mean FD\n",
    "UCLA_CNP_mean_FD.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will drop any participants with a mean FD > 0.55mm per the 'lenient' threshold criteria and save the list of participants to drop to a .txt file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify any participants with Mean_FD_Power > 0.55 and write their sample IDs to a text file\n",
    "UCLA_CNP_participants_to_drop = UCLA_CNP_mean_FD[UCLA_CNP_mean_FD['Mean_FD_Power'] > 0.55]['Sample_ID']\n",
    "UCLA_CNP_participants_to_drop.to_csv('data/input_data/UCLA_CNP_participants_to_drop_lenient.txt', index=False, header=False)\n",
    "\n",
    "ABIDE_participants_to_drop = ABIDE_mean_FD[ABIDE_mean_FD['Mean_FD_Power'] > 0.55]['Sample_ID']\n",
    "ABIDE_participants_to_drop.to_csv('data/input_data/ABIDE_participants_to_drop_lenient.txt', index=False, header=False)\n",
    "\n",
    "UCLA_CNP_participants_to_drop = UCLA_CNP_participants_to_drop.tolist()\n",
    "ABIDE_participants_to_drop = ABIDE_participants_to_drop.tolist()\n",
    "ABIDE_participants_to_drop = [str(x) for x in ABIDE_participants_to_drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will start with our resting-state fMRI data stored in a [`.feather` file](https://arrow.apache.org/docs/python/feather.html) (for easy conversion between R and Python).\n",
    "Data should be organized in a long format, such that there is one row for each brain region and timepoint per participant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input time-series feather files for the two datasets\n",
    "\n",
    "UCLA_CNP_input_time_series_data = pd.read_feather('data/input_data/UCLA_CNP_AROMA_2P_GMR_fMRI_TS.feather')\n",
    "ABIDE_input_time_series_data = pd.read_feather('data/input_data/ABIDE_FC1000_fMRI_TS.feather')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the first five rows of this time-series dataset for the UCLA CNP cohort:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UCLA_CNP_input_time_series_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data should be structured such that there are five columns:\n",
    "1. `Sample_ID`: The unique ID mapping to an individual participant.\n",
    "2. `Noise_Proc`: Name of the noise processing procedure, useful for when multiple noise processing pipelines are evaluated.\n",
    "3. `Brain_Region`: The name of the brain region.\n",
    "4. `timepoint`: The timepoint corresponding to the BOLD frame.\n",
    "5. `values`: The BOLD signal amplitude at the given timepoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting univariate time-series features\n",
    "\n",
    "First, we will extract 25 univariate time-series features comprising the [`catch22`](https://doi.org/10.1007/s10618-019-00647-x) feature set, mean, standard deviation, and fractional amplitude of low-frequency fluctuations (fALFF).\n",
    "The `catch22` features, mean, and SD can all be computed in R using the [`theft`](https://cran.r-project.org/web/packages/theft/vignettes/theft.html) package (collectively referred to as the `catch24` feature set), while the fALFF will be computed in Matlab.\n",
    "Computing the `catch24` features will take several minutes, so feel free to hit play on the next code chunk and grab a coffee ☕️\n",
    "(Alternatively, you can run this on a high-performance computing cluster if you prefer.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i UCLA_CNP_input_time_series_data,ABIDE_input_time_series_data -o UCLA_CNP_catch24_features,ABIDE_catch24_features\n",
    "# Load the theft and tidyr packages\n",
    "library(theft)\n",
    "library(tidyr)\n",
    "\n",
    "# We can define a helper function to compute the `catch24` time-series features using the `theft` package\n",
    "catch24_all_samples <- function(full_TS_data,\n",
    "                                output_column_names = c(\"Output\"),\n",
    "                                unique_columns = c(\"Sample_ID\", \"Brain_Region\", \"Noise_Proc\")) {\n",
    "  \n",
    "  \n",
    "  # Merge columns into unique ID\n",
    "  full_TS_data <- full_TS_data %>%\n",
    "    tidyr::unite(\"Unique_ID\", unique_columns, sep=\"__\")\n",
    "  \n",
    "  # Compute the set of 24 time-series features using theft\n",
    "  TS_catch24 <- theft::calculate_features(data = full_TS_data, \n",
    "                                          id_var = \"Unique_ID\", \n",
    "                                          time_var = \"timepoint\", \n",
    "                                          values_var = \"values\", \n",
    "                                          feature_set = \"catch22\",\n",
    "                                          catch24 = TRUE)[[1]] %>%\n",
    "    tidyr::separate(\"id\", c(output_column_names), sep=\"__\")\n",
    "\n",
    "  # Return the resulting set of 24 features computed per brain region\n",
    "  return(TS_catch24)\n",
    "    \n",
    "}\n",
    "\n",
    "# Compute the 24 time-series features for UCLA CNP and ABIDE time-series data\n",
    "UCLA_CNP_catch24_features <- catch24_all_samples(UCLA_CNP_input_time_series_data,\n",
    "                                                 output_column_names = c(\"Sample_ID\", \"Brain_Region\", \"Noise_Proc\"))\n",
    "ABIDE_catch24_features <- catch24_all_samples(ABIDE_input_time_series_data,\n",
    "                                                output_column_names = c(\"Sample_ID\", \"Brain_Region\", \"Noise_Proc\"))\n",
    "                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will separately compute the fractional amplitude of low-frequency fluctuations (fALFF) using the Matlab script `compute_regional_fALFF.m` as follows (note: Matlab license is required to run this):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in catch24 data\n",
    "UCLA_CNP_catch24_features = pd.read_feather('data/time_series_features/UCLA_CNP_catch24_features.feather')\n",
    "ABIDE_catch24_features = pd.read_feather('data/time_series_features/ABIDE_catch24_features.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "# First, we need to convert our time-series feather file to a Matlab .mat file to be read in properly\n",
    "UCLA_CNP_time_series_file_base='data/input_data/UCLA_CNP_AROMA_2P_GMR_fMRI_TS'\n",
    "ABIDE_time_series_file_base='data/input_data/ABIDE_ASD_FC1000_fMRI_TS'\n",
    "\n",
    "# Run the feather_to_mat.py script with the file base as the input argument, indicating that the output file should be a mat file\n",
    "python3 code/feature_extraction/feather_to_mat.py ${UCLA_CNP_time_series_file_base} mat\n",
    "python3 code/feature_extraction/feather_to_mat.py ${ABIDE_time_series_file_base} mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "# Run the feather_to_mat.py script with the file base as the input argument, indicating that the output file should be a mat file\n",
    "python3 code/feature_extraction/feather_to_mat.py data/input_data/UCLA_CNP_AROMA_2P_GMR_fMRI_TS mat\n",
    "python3 code/feature_extraction/feather_to_mat.py data/input_data/ABIDE_ASD_FC1000_fMRI_TS mat\n",
    "\n",
    "# Define the path to the data\n",
    "data_path=$(echo $(pwd) | tr -d ' ')\n",
    "\n",
    "# Run the compute_regional_fALFF.m script -- note that you might need to update your matlab path here\n",
    "cd code/feature_extraction\n",
    "\n",
    "# UCLA CNP\n",
    "TS_mat_file=\"$data_path/data/input_data/UCLA_CNP_AROMA_2P_GMR_fMRI_TS.mat\"\n",
    "output_mat_file=\"data/time_series_features/UCLA_CNP_fALFF.mat\"\n",
    "matlab -nodisplay -singleCompThread -r \"compute_regional_fALFF $data_path $TS_mat_file $output_mat_file; exit\"\n",
    "\n",
    "# ABIDE\n",
    "TS_mat_file=\"data/input_data/ABIDE_ASD_FC1000_fMRI_TS.mat\"\n",
    "output_mat_file=\"data/time_series_features/ABIDE_fALFF.mat\"\n",
    "matlab -nodisplay -singleCompThread -r \"compute_regional_fALFF $data_path $TS_mat_file $output_mat_file; exit\"\n",
    "\n",
    "# Convert the mat file back to feather for fALFF\n",
    "python3 feather_to_mat.py ${data_path}/data/time_series_features/UCLA_CNP_fALFF feather\n",
    "python3 feather_to_mat.py ${data_path}/data/time_series_features/ABIDE_fALFF feather\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the fALFF feather files\n",
    "UCLA_CNP_fALFF = pd.read_feather('data/time_series_features/UCLA_CNP_fALFF.feather')\n",
    "ABIDE_fALFF = pd.read_feather('data/time_series_features/ABIDE_fALFF.feather')\n",
    "\n",
    "# Remove whitespace from Brain_Region column in the fALFF dataframes\n",
    "UCLA_CNP_fALFF['Brain_Region'] = UCLA_CNP_fALFF['Brain_Region'].str.strip()\n",
    "ABIDE_fALFF['Brain_Region'] = ABIDE_fALFF['Brain_Region'].str.strip()\n",
    "\n",
    "# Read in metadata files\n",
    "UCLA_CNP_metadata = pd.read_feather('data/input_data/UCLA_CNP_sample_metadata.feather')\n",
    "ABIDE_metadata = pd.read_feather('data/input_data/ABIDE_sample_metadata.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i UCLA_CNP_fALFF,ABIDE_fALFF,UCLA_CNP_catch24_features,ABIDE_catch24_features,UCLA_CNP_metadata,ABIDE_metadata,UCLA_CNP_participants_to_drop,ABIDE_participants_to_drop -o UCLA_CNP_catch25_filtered,ABIDE_catch25_filtered\n",
    "\n",
    "source(\"code/feature_extraction/QC_functions_univariate.R\")\n",
    "univariate_feature_set <- \"catch25\"\n",
    "\n",
    "UCLA_CNP_catch25_filtered <- run_QC_for_univariate_dataset(sample_metadata = UCLA_CNP_metadata,\n",
    "                                                           univariate_feature_set = univariate_feature_set,\n",
    "                                                           catch24_results = UCLA_CNP_catch24_features,\n",
    "                                                           fALFF_results = UCLA_CNP_fALFF,\n",
    "                                                           participants_to_drop = UCLA_CNP_participants_to_drop)\n",
    "\n",
    "\n",
    "ABIDE_catch25_filtered <- run_QC_for_univariate_dataset(sample_metadata = ABIDE_metadata,\n",
    "                                                       univariate_feature_set = univariate_feature_set,\n",
    "                                                       catch24_results = ABIDE_catch24_features,\n",
    "                                                       fALFF_results = ABIDE_fALFF,\n",
    "                                                       participants_to_drop = ABIDE_participants_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the filtered data to feather files\n",
    "UCLA_CNP_catch25_filtered.reset_index().to_feather('data/time_series_features/UCLA_CNP_catch25_filtered.feather')\n",
    "ABIDE_catch25_filtered.reset_index().to_feather('data/time_series_features/ABIDE_catch25_filtered.feather')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting statistics of pairwise interactions (SPIs) for coupling strengths\n",
    "\n",
    "For `pyspi` computations, we opted to run a distributed version on a high-performance computing cluster.\n",
    "We parallelized computations by separating the fMRI time-series data into individualized `numpy` array files (`.npy`) to be parsed by individual HPC job nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to iterate over each Sample_ID in the given time series dataframe and save the corresponding time series to a numpy file\n",
    "def split_MTS_into_npy(time_series_data, study):\n",
    "\n",
    "    # Create output directory if it doesn't already exist\n",
    "    try:\n",
    "        os.makedirs(\"data/time_series_features/\" + study, \n",
    "                    exist_ok = True)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    # Iterate over each participant\n",
    "    for sample_id in time_series_data['Sample_ID'].unique().tolist():\n",
    "       # Extract just that participant's data\n",
    "        participant_TS_data = time_series_data.query('Sample_ID == @sample_id')\n",
    "\n",
    "        # Reshape data from long to wide\n",
    "        participant_TS_data_wide = participant_TS_data.pivot(index='Brain_Region', columns='timepoint', values='values').reset_index()\n",
    "\n",
    "        # Convert to numpy array\n",
    "        participant_TS_data_np = participant_TS_data_wide.drop(['Brain_Region'], axis=1).to_numpy()\n",
    "\n",
    "        # Z-score each row\n",
    "        data_norm = np.apply_along_axis(stats.zscore, 1, participant_TS_data_np)\n",
    "\n",
    "        # Save the numpy array to a numpy binary file\n",
    "        np.save('data/time_series_features/' + study + '/' + sample_id + '.npy', data_norm)\n",
    "\n",
    "# Call the function for UCLA_CNP and ABIDE\n",
    "split_MTS_into_npy(UCLA_CNP_input_time_series_data, 'UCLA_CNP')\n",
    "split_MTS_into_npy(ABIDE_input_time_series_data, 'ABIDE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create a `.yaml` file for each study, containing information about the file location for each sample ID, along with a label containing diagnostic group.\n",
    "We specify that the dimension order is presented with brain regions ('processes') as rows and timepoints as columns by setting `dim_order` to `ps`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ID_var='Sample_ID'\n",
    "label_vars='Diagnosis'\n",
    "\n",
    "# Run for UCLA CNP\n",
    "data_dir='data/time_series_features/UCLA_CNP/'\n",
    "sample_metadata_file='data/input_data/UCLA_CNP_sample_metadata.feather'\n",
    "\n",
    "Rscript code/feature_extraction/create_yaml_for_samples.R  \\\n",
    "    --data_dir $data_dir \\\n",
    "    --sample_metadata_file $sample_metadata_file \\\n",
    "    --ID_var $ID_var \\\n",
    "    --label_vars $label_vars \\\n",
    "    --dim_order ps\n",
    "\n",
    "# Run for ABIDE\n",
    "data_dir='data/time_series_features/ABIDE/'\n",
    "sample_metadata_file='data/input_data/ABIDE_sample_metadata.feather'\n",
    "\n",
    "Rscript code/feature_extraction/create_yaml_for_samples.R  \\\n",
    "    --data_dir $data_dir \\\n",
    "    --sample_metadata_file $sample_metadata_file \\\n",
    "    --ID_var $ID_var \\\n",
    "    --label_vars $label_vars \\\n",
    "    --dim_order ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend that you clone the [pyspi-distribute](https://github.com/olivercliff/pyspi-distribute) repository to make use of job distribution on an HPC cluster.\n",
    "To do so, you can clone with the following command:\n",
    "```\n",
    "git clone https://github.com/olivercliff/pyspi-distribute.git\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pyspi_distribute_path='../pyspi-distribute/' # Change to the path where you cloned the pyspi-distribute repo\n",
    "conda_env='base' # Change to the name of the conda environment you want to use\n",
    "queue='normal' # Change to the name of the queue you want to use for your HPC\n",
    "pyspi_walltime_hrs=2 # Change to the maximum walltime in hours you want to allow for each job\n",
    "pyspi_ncpus=1 # Change to the number of CPUs you want to use for each job\n",
    "pyspi_mem=20 # Change to the amount of memory in GB you want to use for each job\n",
    "\n",
    "# UCLA CNP\n",
    "python3 ${pyspi_distribute_path}/distribute_jobs.py \\\n",
    "--data_dir data/time_series_features/UCLA_CNP/ \\\n",
    "--calc_file_name calc.pkl \\\n",
    "--compute_file ${pyspi_distribute_path}/pyspi_compute.py \\\n",
    "--template_pbs_file code/feature_extraction/template_pyspi_distribute.pbs \\\n",
    "--pyspi_config code/feature_extraction/SPIs_14_config.yaml \\\n",
    "--sample_yaml data/time_series_features/UCLA_CNP/sample.yaml \\\n",
    "--pbs_notify a \\\n",
    "--email [your_email_address_here] \\\n",
    "--conda_env $conda_env \\\n",
    "--queue $queue \\\n",
    "--walltime_hrs $pyspi_walltime_hrs \\\n",
    "--cpu $pyspi_ncpus \\\n",
    "--mem $pyspi_mem \\\n",
    "--table_only\n",
    "\n",
    "# ABIDE\n",
    "python3 ${pyspi_distribute_path}/distribute_jobs.py \\\n",
    "--data_dir data/time_series_features/ABIDE/ \\\n",
    "--calc_file_name calc.pkl \\\n",
    "--compute_file ${pyspi_distribute_path}/pyspi_compute.py \\\n",
    "--template_pbs_file code/feature_extraction/template_pyspi_distribute.pbs \\\n",
    "--pyspi_config code/feature_extraction/SPIs_14_config.yaml \\\n",
    "--sample_yaml data/time_series_features/ABIDE/sample.yaml \\\n",
    "--pbs_notify a \\\n",
    "--email [your_email_address_here] \\\n",
    "--conda_env $conda_env \\\n",
    "--queue $queue \\\n",
    "--walltime_hrs $pyspi_walltime_hrs \\\n",
    "--cpu $pyspi_ncpus \\\n",
    "--mem $pyspi_mem \\\n",
    "--table_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running this for all participants, you should have a folder for each participant under `data/time_series_features/[Study]/[Sample_ID]` containing the following files:\n",
    "- `pyspi_run.pbs`: This is the job submission script that was automatically created and submitted for this participant.\n",
    "- `pyspi_run.out`: Notes where the `.npy` file was loaded from and where the resulting `calc.pkl` file was saved for this participant.\n",
    "- `pbsjob.out`: All outputs printed to `stdout`, and `stderr` if applicable. Useful for debugging/confirming that everything ran okay.\n",
    "- `calc.pkl`: pyspi computation results for this participant, saved as a `pandas DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_calc_table = pd.read_pickle('data/time_series_features/UCLA_CNP/sub-70086/calc.pkl')\n",
    "example_calc_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the above example table, the results are stored in a long format such that there is a `brain_region_from` and a `brain_region_to`, an `SPI` (in this case, `cov_EmpiricalCovariance` which is equivalent to the Pearson correlation coefficient) and a `value`.\n",
    "Note that the `value` is `NaN` in the first row since that corresponds to a self-connection, which is not a valid measurement (and will subsequently be dropped when we merge the results).\n",
    "The default output value nomenclature for `pyspi` is process IDs, such as `proc-0`, `proc-1`, etc., which we will map to brain region names subsequently, too.\n",
    "\n",
    "Merging and data cleaning is accomplished with the script `code/feature_extraction/merge_pyspi_data.py`, which can be run as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "python3 code/feature_extraction/merge_pyspi_data.py \\\n",
    "--data_path data/ \\\n",
    "--dataset_ID UCLA_CNP \\\n",
    "--pkl_file calc.pkl \\\n",
    "--pairwise_feature_set pyspi14 \\\n",
    "--brain_region_lookup data/input_data/UCLA_CNP_Brain_Region_Lookup.feather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should have the following files in your `data/time_series_features/` path for subsequent classification analysis:\n",
    "- `ABIDE_catch25_filtered.feather`\n",
    "- `ABIDE_pyspi14_filtered.feather`\n",
    "- `UCLA_CNP_catch25_filtered.feather`\n",
    "- `UCLA_CNP_pyspi14_filtered.feather`\n",
    "\n",
    "\n",
    "Our last step will be to compare across the univariate and pairwise feature sets for each dataset and find the participants with data available for both feature sets as a quality control check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_univariate_pairwise(data_path, dataset_ID, univariate_feature_set, pairwise_feature_set):\n",
    "\n",
    "    # Load in data on samples with univariate feature data\n",
    "    filtered_univariate_data = pd.read_feather(f\"{data_path}/{dataset_ID}_{univariate_feature_set}_filtered.feather\")\n",
    "    filtered_univariate_samples = pd.DataFrame(filtered_univariate_data.Sample_ID.unique(), columns=[\"Sample_ID\"])\n",
    "    \n",
    "    # Load in data on samples with pairwise feature data\n",
    "    filtered_pairwise_data = pd.read_feather(f\"{data_path}/{dataset_ID}_{pairwise_feature_set}_filtered.feather\")\n",
    "    filtered_pairwise_samples = pd.DataFrame(filtered_pairwise_data.Sample_ID.unique(), columns=[\"Sample_ID\"])\n",
    "    \n",
    "    # Merge the two datasets\n",
    "    merged_sample_info = pd.merge(filtered_univariate_samples, filtered_pairwise_samples, how=\"inner\")\n",
    "    merged_sample_info.to_feather(f\"{data_path}/{dataset_ID}_filtered_sample_info_{univariate_feature_set}_{pairwise_feature_set}.feather\")\n",
    "    \n",
    "# Run for UCLA CNP\n",
    "intersection_univariate_pairwise(data_path='data/time_series_features',\n",
    "                                 dataset_ID='UCLA_CNP',\n",
    "                                 univariate_feature_set='catch25',\n",
    "                                 pairwise_feature_set='pyspi14')\n",
    "\n",
    "# Run for ABIDE\n",
    "intersection_univariate_pairwise(data_path='data/time_series_features',\n",
    "                                 dataset_ID='ABIDE',\n",
    "                                 univariate_feature_set='catch25',\n",
    "                                 pairwise_feature_set='pyspi14')\n",
    "\n",
    "# Apply this final filter to the .feather files\n",
    "UCLA_CNP_filtered_sample_info = pd.read_feather('data/time_series_features/UCLA_CNP_filtered_sample_info_catch25_pyspi14.feather')\n",
    "\n",
    "UCLA_CNP_catch25_filtered = pd.read_feather('data/time_series_features/UCLA_CNP_catch25_filtered.feather').query(\"Sample_ID in @UCLA_CNP_filtered_sample_info.Sample_ID.unique().tolist()\")\n",
    "UCLA_CNP_catch25_filtered.reset_index().to_feather('data/time_series_features/UCLA_CNP_catch25_filtered.feather')\n",
    "\n",
    "UCLA_CNP_pyspi14_filtered = pd.read_feather('data/time_series_features/UCLA_CNP_pyspi14_filtered.feather').query(\"Sample_ID in @UCLA_CNP_filtered_sample_info.Sample_ID.unique().tolist()\")\n",
    "UCLA_CNP_pyspi14_filtered.reset_index().to_feather('data/time_series_features/UCLA_CNP_pyspi14_filtered.feather')\n",
    "\n",
    "ABIDE_filtered_sample_info = pd.read_feather('data/time_series_features/ABIDE_filtered_sample_info_catch25_pyspi14.feather')\n",
    "\n",
    "ABIDE_catch25_filtered = pd.read_feather('data/time_series_features/ABIDE_catch25_filtered.feather').query(\"Sample_ID in @ABIDE_filtered_sample_info.Sample_ID.unique().tolist()\")\n",
    "ABIDE_catch25_filtered.reset_index().to_feather('data/time_series_features/ABIDE_catch25_filtered.feather')\n",
    "\n",
    "ABIDE_pyspi14_filtered = pd.read_feather('data/time_series_features/ABIDE_pyspi14_filtered.feather').query(\"Sample_ID in @ABIDE_filtered_sample_info.Sample_ID.unique().tolist()\")\n",
    "ABIDE_pyspi14_filtered.reset_index().to_feather('data/time_series_features/ABIDE_pyspi14_filtered.feather')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
