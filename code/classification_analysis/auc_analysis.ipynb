{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "\n",
    "data_path=\"/headnode1/abry4213/data/TS_feature_manuscript/\"\n",
    "sys.path.insert(0, './')\n",
    "from core_classification_functions import *\n",
    "from mixed_sigmoid_normalisation import MixedSigmoidScaler\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score\n",
    "from copy import deepcopy\n",
    "# %load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "UCLA_CNP_subjects_to_keep = pd.read_feather(f\"{data_path}/time_series_features/UCLA_CNP_filtered_sample_info_catch25_pyspi14.feather\")\n",
    "ABIDE_subjects_to_keep = pd.read_feather(f\"{data_path}/time_series_features/ABIDE_filtered_sample_info_catch25_pyspi14.feather\")\n",
    "\n",
    "# Load metadata\n",
    "UCLA_CNP_metadata = (pd.read_feather(f\"{data_path}/input_data/UCLA_CNP_sample_metadata.feather\")\n",
    "                        .assign(Study = \"UCLA_CNP\")\n",
    "                        .query(\"Sample_ID in @UCLA_CNP_subjects_to_keep.Sample_ID\"))\n",
    "ABIDE_metadata = (pd.read_feather(f\"{data_path}/input_data/ABIDE_sample_metadata.feather\")\n",
    "                        .assign(Study = \"ABIDE\")\n",
    "                        .query(\"Sample_ID in @ABIDE_subjects_to_keep.Sample_ID\"))\n",
    "\n",
    "# Load univariate time-series feature info\n",
    "univariate_feature_info = pd.read_csv(f\"{data_path}/feature_info/univariate_feature_info.csv\")\n",
    "pairwise_feature_info = pd.read_csv(f\"{data_path}/feature_info/pairwise_feature_info.csv\")\n",
    "\n",
    "# Define parameters that you can change\n",
    "univariate_feature_set = \"catch25\"\n",
    "pairwise_feature_set = \"pyspi14\"\n",
    "\n",
    "# Load SPI directionality information\n",
    "SPI_directionality_data = pd.read_csv(\"SPI_Direction_Info.csv\")\n",
    "SPI_directionality_dict = dict(SPI_directionality_data.values)\n",
    "\n",
    "# Load univariate time-series feature data for the two datasets\n",
    "UCLA_CNP_univariate_features = pd.read_feather(f\"{data_path}/time_series_features/UCLA_CNP_catch25_filtered.feather\")\n",
    "# ABIDE_univariate_features = pd.read_feather(f\"{data_path}/time_series_features/ABIDE_catch25_filtered.feather\")\n",
    "\n",
    "# # Load pyspi14 data for UCLA CNP and ABIDE\n",
    "# UCLA_CNP_pyspi14 = pd.read_feather(f\"{data_path}/time_series_features/UCLA_CNP_pyspi14_filtered.feather\")\n",
    "# ABIDE_pyspi14 = pd.read_feather(f\"{data_path}/time_series_features/ABIDE_pyspi14_filtered.feather\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/headnode1/abry4213/.conda/envs/annie_env/lib/python3.9/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = svm.SVC(kernel=\"linear\", C=1, class_weight=\"balanced\", probability=True)\n",
    "pipe = Pipeline([('scaler', MixedSigmoidScaler(unit_variance=True)),\n",
    "                ('model', model)])\n",
    "classifier_type = \"Linear_SVM_sklearn\"\n",
    "\n",
    "# Define scorers\n",
    "scorers = [make_scorer(roc_auc_score, needs_proba=True), make_scorer(balanced_accuracy_score), make_scorer(accuracy_score)]\n",
    "scoring_names = [\"AUC\", \"Balanced_Accuracy\", \"Accuracy\"]\n",
    "\n",
    "# Classification parameters\n",
    "num_folds = 10\n",
    "num_repeats = 10\n",
    "num_null_iters = 0\n",
    "num_jobs = 1\n",
    "RepeatedStratifiedKFold_splitter = RepeatedStratifiedKFold(n_splits=num_folds, n_repeats=num_repeats, random_state=127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "disorder = \"ADHD\"\n",
    "dataset_ID = \"UCLA_CNP\"\n",
    "\n",
    "class_labels = np.load(f\"{data_path}/input_data/{dataset_ID}_{disorder}_class_labels.npy\")\n",
    "sample_IDs = np.load(f\"{data_path}/input_data/{dataset_ID}_{disorder}_sample_IDs.npy\")\n",
    "\n",
    "model_name = \"UCLA_CNP_ADHD_catch25_feature_SP_Summaries_welch_rect_area_5_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running UCLA_CNP_ADHD_catch25_feature_SP_Summaries_welch_rect_area_5_1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running {model_name}\")\n",
    "# Define analysis type\n",
    "if \"ROI\" in model_name:\n",
    "    Analysis_Type = \"Brain_Region\"\n",
    "elif \"combo_catch25_features_all_regions\" in model_name:\n",
    "    Analysis_Type = \"Univariate_Combo\"\n",
    "elif \"combined_univariate_catch25_and_pyspi14\" in model_name:\n",
    "    Analysis_Type = \"SPI_Combo\"\n",
    "elif \"catch25_feature\" in model_name:\n",
    "    Analysis_Type = \"catch25_feature\"\n",
    "else:\n",
    "    Analysis_Type = \"pyspi14_SPI\"\n",
    "\n",
    "# Find grouping_var\n",
    "if Analysis_Type==\"Brain_Region\":\n",
    "    grouping_var = model_name.split(\"_ROI_\")[1]\n",
    "elif Analysis_Type==\"Univariate_Combo\":\n",
    "    grouping_var = \"Combo\"\n",
    "elif Analysis_Type == \"SPI_Combo\":\n",
    "    grouping_var = model_name.split(\"combined_univariate_catch25_and_pyspi14_SPI_\")[1]\n",
    "elif Analysis_Type == \"catch25_feature\":\n",
    "    grouping_var = model_name.split(\"_catch25_feature_\")[1]\n",
    "else:\n",
    "    grouping_var = model_name.split(\"_pyspi14_SPI_\")[1]\n",
    "\n",
    "feature_data = np.load(f\"{data_path}/time_series_features/processed_numpy_files/{model_name}.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find splits\n",
    "splits = list(RepeatedStratifiedKFold_splitter.split(feature_data, class_labels))\n",
    "# Convert splits to a dataframe\n",
    "splits_df = pd.DataFrame(splits, columns = [\"Train\", \"Test\"])\n",
    "\n",
    "# Assign the fold and repeat numbers\n",
    "splits_df[\"Fold\"] = splits_df.index % num_folds\n",
    "splits_df[\"Repeat\"] = splits_df.index // num_repeats\n",
    "\n",
    "fold_res_list = []\n",
    "class_1_col_list = []\n",
    "\n",
    "# Iterate over each row of fold_splits dataframe\n",
    "for i, row in splits_df.iterrows():\n",
    "    fold_num = row[\"Fold\"]\n",
    "    repeat_num = row[\"Repeat\"]\n",
    "    train_indices = row[\"Train\"]\n",
    "    test_indices = row[\"Test\"]\n",
    "\n",
    "    train_data = feature_data[train_indices]\n",
    "    test_data = feature_data[test_indices]\n",
    "\n",
    "    train_data = feature_data[train_indices]\n",
    "    test_data = feature_data[test_indices]\n",
    "\n",
    "    train_labels = class_labels[train_indices]\n",
    "    test_labels = class_labels[test_indices]\n",
    "\n",
    "    # Fit pipe to train_data\n",
    "    loop_pipe = deepcopy(pipe)\n",
    "    loop_pipe.fit(train_data, train_labels)\n",
    "    test_preds = loop_pipe.predict(test_data)\n",
    "    test_preds_prob = loop_pipe.predict_proba(test_data)\n",
    "\n",
    "    # Find out which column corresponds to which class\n",
    "    fitted_model_classes = loop_pipe.classes_\n",
    "    class_1_col = np.where(fitted_model_classes==1)[0][0]\n",
    "    test_preds_prob_data = test_preds_prob[:,class_1_col]\n",
    "    class_1_col_list.append(class_1_col)\n",
    "\n",
    "    # # Figure out which column to keep: \n",
    "    # if len(test_preds[test_preds==0]) == 0:\n",
    "    #     # Find whichever column of test_preds_prob has the lower mean\n",
    "    #     prob_col = np.argmin([np.mean(test_preds_prob[:,0]), np.mean(test_preds_prob[:,1])])\n",
    "    # elif len(test_preds[test_preds==1]) == 0:\n",
    "    #     # Find whichever column of test_preds_prob has the higher mean\n",
    "    #     prob_col = np.argmax([np.mean(test_preds_prob[:,0]), np.mean(test_preds_prob[:,1])])\n",
    "    # elif np.mean(test_preds_prob[test_preds==0]) < 0.5: \n",
    "    #     prob_col = 1\n",
    "    # else: \n",
    "    #     prob_col = 0\n",
    "    # test_preds_prob_data = test_preds_prob[:,prob_col]\n",
    "\n",
    "    # Compute AUC\n",
    "    auc = roc_auc_score(test_labels, test_preds_prob_data)\n",
    "\n",
    "    this_loop_res = pd.DataFrame({\"Fold\": fold_num, \"Repeat\": repeat_num, \"AUC\": auc}, index=[0])\n",
    "    fold_res_list.append(this_loop_res)\n",
    "\n",
    "all_AUC_fold_res = pd.concat(fold_res_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
